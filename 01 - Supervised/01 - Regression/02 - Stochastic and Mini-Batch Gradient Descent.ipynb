{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "We calculate the loss and gradient based on only **one sample**.   (Not recommended due to unstability)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = (h^{(i)}-y^{(i)})x_j$$\n",
    "\n",
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "We calculate the loss and gradient based on **subset of samples**.  (Recommended; used as standard in deep learning)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=start}^{batch}(h^{(i)}-y^{(i)})x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# actually you can do like this too\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I want to demonstrate some techniques:\n",
    "- **Class** - it's much better to write any in the class form, as you can modularize it for future use, so please be comfortable with it\n",
    "- **Early stopping** - it's expensive to run the model until a certain set of iterations; instead, we can stop when the training loss is no longer decreasing\n",
    "- **Cross validation** - we never really do cross validation from scratch, so here I showed you how to do it\n",
    "\n",
    "Some terms worth mentioning:\n",
    "- **Epoch** - it's a popular term in ML/DL - one epoch refers to a training and validation process with all the training data with one model.\n",
    "\n",
    "Some coding practice worth mentioning:\n",
    "- Notice how I put `_` in front of some function; it does not really have any functionality aside from telling the coders that it is not meant for outside use (should not be called from `__main__`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    #in this class, we add cross validation as well for some spicy code....\n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001, num_epochs=5, batch_size=50, method='batch', cv=kfold):\n",
    "        self.alpha      = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "    \n",
    "    def mse(self, ytrue, ypred):\n",
    "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #create a list of kfold scores\n",
    "        self.kfold_scores = list()\n",
    "        \n",
    "        #reset val loss\n",
    "        self.val_loss_old = np.infty\n",
    "\n",
    "        #kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "                    \n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            \n",
    "            #early stopping\n",
    "            val_loss_new = self.mse(y_cross_val, yhat_val)\n",
    "            if np.allclose(val_loss_new, self.val_loss_old):\n",
    "                break\n",
    "            self.val_loss_old = val_loss_new\n",
    "            \n",
    "            self.kfold_scores.append(val_loss_new)\n",
    "            print(f\"Fold {fold}: {val_loss_new}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "        self.theta = self.theta - self.alpha * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercept\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "    def _bias(self):\n",
    "        return self.theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 4995.642541794574\n",
      "Fold 1: 4035.270261440947\n",
      "Fold 2: 5465.4331522548455\n",
      "Fold 3: 5116.4382315849225\n",
      "Fold 4: 3580.4397606397456\n",
      "Test MSE:  3789.9012437030083\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(method=\"batch\") #<==try put method=\"batch\" or \"sto\"\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "mse  = lr.mse(yhat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"Test MSE: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Workshop - Check your understandings\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "Instruction:  Gather in your group.  Will randomly pick groups to present.\n",
    "\n",
    "1.  Using the table of egg price, gold price, etc., please do the followings:\n",
    "    1.  Draw the vector $\\mathbf{x}_2$\n",
    "    2.  Draw the vector $\\mathbf{x}^{(2)}$\n",
    "    3.  What is $x^{(2)}_1$\n",
    "    4.  What is $y^{(1)}$\n",
    "    5.  Draw the vector $\\mathbf{y}$\n",
    "    6.  Draw the entire matrix $\\mathbf{X}$\n",
    "    7.  Create the vector $\\mathbf{x}_2$ using numpy\n",
    "    8.  Create the matrix $\\mathbf{X}$ using numpy\n",
    "2.  Given the first sample $\\mathbf{x}^{(1)}$, and given a weight $\\boldsymbol{\\theta}$ of [2, 0.1, 3], what is $\\hat{y}$? Show the calculation by hand.  (this is called *vectorized* form)\n",
    "3.  Perform 2 using numpy\n",
    "4.  Given the entire matrix $\\mathbf{X}$, and using the same weight as above, what is $\\hat{\\mathbf{y}}$?  Show the calculation by hand.  This is called *matrix* form.\n",
    "5.  Perform 4 using numpy\n",
    "6.  Given the entire matrix $\\mathbf{X}$, given a weight $\\boldsymbol{\\theta}$ of [2, 0.1, 3], and given the $\\mathbf{y}$, what is $J(\\theta)$? Show the calculation by hand.\n",
    "7.  Perform 6 using numpy\n",
    "8.  For training sample 1, i.e., $\\mathbf{x}^{(1)}$, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta_2}$  Show the calculation by hand.\n",
    "9.  For all samples, i.e., $\\mathbf{X}$, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta_2}$  Show the calculation by hand.\n",
    "10. For all samples, what is $\\displaystyle \\frac{\\partial J}{\\partial \\theta}$.  Show the calculation by hand.\n",
    "11. Given $\\alpha = 0.01$, what is the new $\\boldsymbol{\\theta}$?\n",
    "12. In the beginning of gradient descent, what is the initial weight?\n",
    "13. Review how to find derivatives/gradients, especially multiplication and chain rule by yourself.\n",
    "14. Explain in your own words, why finding the derivatives can help you find the better weight.\n",
    "15. Explain what is stochastic and mini-batch gradient descent.  (actually what you did in 8 is stochastic)\n",
    "16. Now you have transcended.  Go to the code and understand everything :-)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
