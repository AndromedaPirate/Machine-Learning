{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "We calculate the loss and gradient based on only **one sample**.   (Not recommended due to unstability)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = (h^{(i)}-y^{(i)})x_j$$\n",
    "\n",
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "We calculate the loss and gradient based on **subset of samples**.  (Recommended; used as standard in deep learning)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=start}^{batch}(h^{(i)}-y^{(i)})x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/307462513559759924', creation_time=1689306072647, experiment_id='307462513559759924', last_update_time=1689306072647, lifecycle_stage='active', name='chaky-diabetes-example', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#experiment tracking\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://la.cs.ait.ac.th\")\n",
    "# mlflow.create_experiment(name=\"chaky-diabetes-example\")  #create if you haven't create\n",
    "mlflow.set_experiment(experiment_name=\"chaky-diabetes-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# actually you can do like this too\n",
    "# X = np.insert(X, 0, 1, axis=1)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I want to demonstrate some techniques:\n",
    "- **Class** - it's much better to write any in the class form, as you can modularize it for future use, so please be comfortable with it\n",
    "- **Early stopping** - it's expensive to run the model until a certain set of iterations; instead, we can stop when the training loss is no longer decreasing\n",
    "- **Cross validation** - we never really do cross validation from scratch, so here I showed you how to do it\n",
    "\n",
    "Experiment tracking:\n",
    "- **MLFlow** - this is a popular experiment tracking tool.  Everyone love it and use it.  Here I just demonstrated a very simple usage - please continue to self-study.\n",
    "\n",
    "Some terms worth mentioning:\n",
    "- **Epoch** - it's a popular term in ML/DL - one epoch refers to a training and validation process with all the training data with one model.\n",
    "\n",
    "Some coding practice worth mentioning:\n",
    "- Notice how I put `_` in front of some function; it does not really have any functionality aside from telling the coders that it is not meant for outside use (should not be called from `__main__`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    #in this class, we add cross validation as well for some spicy code....\n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001, num_epochs=5, batch_size=50, method='batch', cv=kfold):\n",
    "        self.alpha      = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.cv         = cv\n",
    "    \n",
    "    def mse(self, ytrue, ypred):\n",
    "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "            \n",
    "        #create a list of kfold scores\n",
    "        self.kfold_scores = list()\n",
    "        \n",
    "        #reset val loss\n",
    "        self.val_loss_old = np.infty\n",
    "\n",
    "        #kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "                    \n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            \n",
    "            #early stopping\n",
    "            val_loss_new = self.mse(y_cross_val, yhat_val)\n",
    "            if np.allclose(val_loss_new, self.val_loss_old):\n",
    "                break\n",
    "            self.val_loss_old = val_loss_new\n",
    "            \n",
    "            self.kfold_scores.append(val_loss_new)\n",
    "            print(f\"Fold {fold}: {val_loss_new}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "        self.theta = self.theta - self.alpha * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercept\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "    def _bias(self):\n",
    "        return self.theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 3990.7435685751593\n",
      "Fold 1: 4924.366277323169\n",
      "Fold 2: 4329.149051087357\n",
      "Fold 3: 4052.190647395748\n",
      "Fold 4: 5648.755870932162\n",
      "Test MSE:  5509.353824615312\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(method=\"batch\") #<==try put method=\"batch\" or \"sto\"\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "mse  = lr.mse(yhat, y_test)\n",
    "\n",
    "# print the mse\n",
    "print(\"Test MSE: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 8820768048950.701\n",
      "Fold 1: 3580841319844.273\n",
      "Fold 2: 6380667824783.314\n",
      "Fold 3: 11227811335787.287\n",
      "Fold 4: 6417509937430.679\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run(run_name=\"experiment-batch\")\n",
    "\n",
    "#######\n",
    "params = {\"method\": \"batch\", \"alpha\": 0.01}\n",
    "lr = LinearRegression(**params) #<==try put method=\"batch\" or \"sto\"\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "mse  = lr.mse(yhat, y_test)\n",
    "\n",
    "mlflow.log_params(params=params)\n",
    "mlflow.log_metric('mse', mse)\n",
    "\n",
    "# mlflow.log_figure(fig.figure_, f\"XXX.png\")  #you can log figure too in case you have one...\n",
    "#mlflow.sklearn.log_model(sk_model, \"sk_models\", signature=signature)  #you can also let mlflow save model\n",
    "\n",
    "#######\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 78837.08682827152\n",
      "Fold 1: 2646.300208375495\n",
      "Fold 2: 2792.062951504067\n",
      "Fold 3: 6530.007454750576\n",
      "Fold 4: 9957.29334330124\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run(run_name=\"experiment-mini\")\n",
    "\n",
    "#######\n",
    "params = {\"method\": \"mini\", \"alpha\": 0.01}\n",
    "lr = LinearRegression(**params)\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "mse  = lr.mse(yhat, y_test)\n",
    "\n",
    "mlflow.log_params(params=params)\n",
    "mlflow.log_metric('mse', mse)\n",
    "\n",
    "#######\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Workshop - Check your understandings\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "Instruction:  Gather in your group.  Will randomly pick groups to present.\n",
    "\n",
    "1.  Explain why Chaky teach stochastic and mini-batch gradient descent.  Why we should care?  Explain in your own words.\n",
    "2.  What's the shape of `X_train` and `y_train`?\n",
    "3.  What does `batch_size=50` means?\n",
    "4.  what is `np.infty`?  Why is set like that?\n",
    "5.  What is `enumerate(self.cv.split(X_train))`\n",
    "6.  What's the shape of `X_cross_train` and `y_cross_train`?\n",
    "7.  What's the shape of `X_method_train` and `y_method_train` in the case of stochastic gradient descent?\n",
    "8.  What's the shape of `X_method_train` and `y_method_train` in the case of mini-batch gradient descent?\n",
    "9.  What's the shape of `X_method_train` and `y_method_train` in the case of batch gradient descent?\n",
    "10. In https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection, there are many ways to split and `KFold` is the simplest.  Try do `enumerate` the `ShuffleSplit` and tells us what index it gave us.   If you are confused, see the examples shown in https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "11. Change our code so that it uses `ShuffleSplit`.  Report the Test MSE.\n",
    "12. Please learn about `TimeSeriesSplit` by yourself, as it is used in time series data.\n",
    "13. In one `epoch`, we train a model using all training data.  Which line(s) of code makes sure we used all the training data?\n",
    "14. What is `np.random.permutation` and what does it give you?  Demonstrate your answer with trial code.\n",
    "15. In a line `X_method_train = X_cross_train[batch_idx].reshape(1, -1)`, why do we need to reshape?\n",
    "16. In a line `yhat_val = self.predict(X_cross_val)`, when should validation happens?  \n",
    "17. What does this line do: `if np.allclose(val_loss_new, self.val_loss_old): break`?\n",
    "18. Explain why the coefficients can be achieved via `return self.theta[1:]`\n",
    "19. Perform an experiment comparing these parameters using cross-validation: `alpha: {0.1, 0.01, 0.001}`, `method:{'batch', 'sto', 'mini'}`.  Write a short paragraph report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
