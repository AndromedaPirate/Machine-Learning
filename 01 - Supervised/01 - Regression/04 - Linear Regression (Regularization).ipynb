{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit-Learn](http://scikit-learn.org) provides quick access to a huge pool of machine learning algorithms.\n",
    "\n",
    "Before using sklearn, there is **one thing you need to know**, i.e., the **data shape that sklearn wants**.\n",
    "\n",
    "To apply majority of the algorithms, sklearn requires two inputs, i.e., $\\mathbf{X}$ and $\\mathbf{y}$.\n",
    "\n",
    "-  $\\mathbf{X}$, or the **feature matrix** *typically* has the shape of ``[n_samples, n_features]``\n",
    "-  $\\mathbf{y}$, or the **target/label vector** *typically* has the shape of ``[n_samples, ]`` or ``[n_samples, n_targets]`` depending whether that algorithm supports multiple labels\n",
    "\n",
    "<img src = \"figures/shape.png\">\n",
    "\n",
    "Note 1:  if you $\\mathbf{X}$ has only 1 feature, the shape must be ``[n_samples, 1]`` NOT ``[n_samples, ]``\n",
    "\n",
    "Note 2:  sklearn supports both numpy and pandas, as long as the shape is right.  For example, if you use pandas, $\\mathbf{X}$ would be a dataframe, and $\\mathbf{y}$ could be a series or dataframe.\n",
    "\n",
    "Tips:  it's always better to look at sklearn documentation before applying any algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of the sklearn API\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn API are as follows:\n",
    "\n",
    "1. Import a class of model\n",
    "2. Choose model hyperparameters by instantiating this class with desired values.\n",
    "3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "4. Fit the model to your data by calling the ``fit()`` method of the model instance.\n",
    "5. Perform inference using the ``predict()`` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try!\n",
    "\n",
    "Before anything, let's load a toy regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import a class of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose model hyperparameters\n",
    "\n",
    "For our linear regression example, we can instantiate the ``LinearRegression`` class and specify that we would like to fit the intercept using the ``fit_intercept`` hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arrange data into a features matrix and target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(X_train.shape) == 2 and len(X_test.shape) == 2  #correct shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(y_train.shape) == 1 and len(y_test.shape) == 1  #correct shape!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit the model to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)  #when you train your model, use your training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``fit()`` command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore.\n",
    "In Scikit-Learn, by convention all model parameters that were learned during the ``fit()`` process have trailing underscores; for example in this linear model, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.87547741,  -9.26843436,  23.24283857,  18.52697371,\n",
       "       -30.72400693,  17.48415588,   2.06334652,   7.45377418,\n",
       "        33.45802239,   2.61129274])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.29126213592232"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two parameters represent the slope and intercept of the simple linear fit to the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict labels for unknown data\n",
    "\n",
    "Once the model is trained, we can now evaluate our model which is called **inference** or **testing**.  Usually we do this with test set (but here we are just lazy for simplicity).  \n",
    "\n",
    "In Scikit-Learn, this can be done using the ``predict()`` method.\n",
    "For the sake of this example, our \"new data\" will be a grid of *x* values, and we will ask what *y* values the model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)  #inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180.1621027594665"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute mean squared error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mean_squared_error(y_true, y_pred)\n",
    "mean_squared_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's very close to what we got before, using our code from scratch, but with 10x fewer lines :-)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Having only two sets, i.e., training and test set is NOT recommended because:\n",
    "\n",
    "1. What if I want to check which hyperparameter is good?  How to check when I should NEVER touch test set?\n",
    "\n",
    "2. What if somehow I got lucky with my split and my training set is very good, and my test set is also very good, just **by chance*?\n",
    " \n",
    "The recommended way is to do **cross-validation**\n",
    "\n",
    "- **Idea**:  further **split the training set into actual training set and validation set**.  To make sure we don't get lucky with our validation set, we do this split either randomly or walkforward like in this picture:\n",
    "\n",
    "<img width=\"400\" src = \"figures/cv.png\" >\n",
    "\n",
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.  This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21229232, 0.45349073, 0.51633375, 0.58399592, 0.64328844])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X_train, y_train, cv=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, if we specify cv with integer, the <code>cross_val_score</code> uses KFold strategies by default (KFold is basically the picture above).  We can also manually specify the CV strategies we want.\n",
    "\n",
    "<img width=\"400\" src = \"figures/kfold.png\">\n",
    "\n",
    "For example, **ShuffleSplit**:\n",
    "\n",
    "ShuffleSplit is a good alternative to KFold cross validation that allows a finer control on the number of iterations and the proportion of samples on each side of the train / test split.\n",
    "\n",
    "<img width=\"400\" src = \"figures/shuffle.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55653483, 0.61272515, 0.49418558, 0.2213325 , 0.40555013])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "shuffle_cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)  #splitting in a randomized way\n",
    "cross_val_score(model, X_train, y_train, cv=shuffle_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common strategy is **Stratified KFold**\n",
    "\n",
    "StratifiedKFold is a variation of k-fold which returns stratified folds: **each set contains approximately the same percentage of samples of each target class**.\n",
    "\n",
    "<img width=\"400\" src = \"figures/skfold.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/DSAI/Environments/teaching_env/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.30962551, 0.5635614 , 0.59665989])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold #mostly used for classification\n",
    "\n",
    "sk_cv = StratifiedKFold(n_splits=3)  #there's also stratified shuffle kfold!\n",
    "cross_val_score(model, X_train, y_train, cv=sk_cv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy is **Group KFold**:\n",
    "\n",
    "Very useful if you don't want the same group in both training and validation set.\n",
    "\n",
    "<img width=\"400\" src = \"figures/groupkfold.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.53971909, 0.44107877, 0.52276067, 0.47754004, 0.4587158 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)  #there's also group shuffle kfold!\n",
    "\n",
    "#we have to specify the group\n",
    "#let's specify, just for the sake\n",
    "groups = np.random.randint(0, 5, size=y_train.shape[0])\n",
    "print(groups.shape)\n",
    "#print(groups)\n",
    "\n",
    "cross_val_score(model, X_train, y_train, cv=gkf, groups=groups) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, read https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "Note: **one big reminder is that cross-validation is for finding optimal hyperparameters/models, you still need to evaluate with final test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double/Nested Cross-validation\n",
    "\n",
    "Recall the two problems we have:\n",
    "\n",
    "1. What if I want to check which hyperparameter is good?  How to check when I should NEVER touch test set?\n",
    "\n",
    "2. What if somehow I got lucky with my split and my training set is very good, and my test set is also very good, just **by chance*?\n",
    "\n",
    "Actually, we solved number 1 only!\n",
    "\n",
    "How about number 2?  Because we may be lucky with our testing set!\n",
    "\n",
    "**Idea: put another loop when we initally split training and test set, i.e., we now have two loops**\n",
    "\n",
    "<img width=\"400\" src = \"figures/nestedcv.png\">\n",
    "\n",
    "**Then the final performance is simply the average of all outerloop performance, instead of testing the model with final test set, because here, we don't have final test set**\n",
    "\n",
    "To do nested cross-validation, let's first learn <code>GridSearch</code> which will be needed for doing nested cross-validation quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Recall that we can compare models/hyperparameters using <code>cross_val_score</code>, right?   But this can be tiring....Scikit-Learn provides automated tools to do this called <code>GridSearchCV</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=5, random_state=0, test_size=0.3, train_size=None),\n",
       "             estimator=LinearRegression(),\n",
       "             param_grid={'fit_intercept': [True, False],\n",
       "                         'positive': [True, False]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'fit_intercept': [True, False],\n",
    "              'positive': [True, False]}\n",
    "\n",
    "#GridSearchCV(algorithm, dictionary, cross-validation strategy)\n",
    "grid = GridSearchCV(model, param_grid, cv=shuffle_cv, refit=True)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is fit, we can ask for the best parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_intercept': True, 'positive': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got the new model, we can test it on the final final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180.1621027594665"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = grid.predict(X_test)  #note that here i can use grid right away, because i specify refit=True\n",
    "mean_squared_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that there is also other form of gridsearchcv such as randomized grid search which can be more efficient.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming back to nested cross-validation\n",
    "\n",
    "Once we learn about grid search, we can utilize both <code>grid search</code> and <code>cross_val_score</code> to perform nested cross validation like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2903.10000132, -3315.33400852, -3144.27434507, -3045.8808507 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#specify the inner cv and outer cv\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=1)\n",
    "\n",
    "#inner loop\n",
    "inner_model = GridSearchCV(model, param_grid=param_grid, cv=inner_cv)\n",
    "\n",
    "#outer loop\n",
    "nested_score = cross_val_score(inner_model, X, y, scoring='neg_mean_squared_error', cv=outer_cv)\n",
    "                              \n",
    "nested_score #higher mean better....(sklearn wants to keep this convention)\n",
    "\n",
    "#see this ==> https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you just average them, and this is your very robust estimates of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3102.1473014039575"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two big problems of nested cross-validation:\n",
    "    \n",
    "1. It is time consuming and resource hungry.\n",
    "2. You no longer know what hyperparameters or best models....because in the inner loop, the model varies....**so yes, nested cross-validation do not give you a final model!! XD**\n",
    "\n",
    "**So how to use nested cross-validation**\n",
    "\n",
    "1. First, use nested cross-validation to look for **model instability**.  If there is a lot of instability, you want to **skip the model or change the search space**.  \n",
    "\n",
    "2. Once you got a model that is very stable, run a typical (simple, no nested) cross-validation to find the best version of that model, so you can deploy.\n",
    "    - you can either train (1) on the entire dataset (if you are super sure), or (2) training set (preferable).\n",
    "\n",
    "**It takes too much time!!!**\n",
    "\n",
    "1. Make the outer loop smaller OR\n",
    "2. Don't use it\n",
    "3. Implement by yourself, and putting breaks to save progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other types of regression\n",
    "\n",
    "Linear regression is the most basic algorithm.  There are many mores.  Today, we shall briefly talked about other types, for the sake of completeness.   Then we shall revisit the diabetes dataset, and compare different regression algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Limitation of simple linear regression comes when we have non-linear data.  We can simply use polynomial regression.  For example, a degree-1 polynomial fits a straight line to the data like this: \n",
    "\n",
    "$$y = ax + b$$  \n",
    "\n",
    "A degree-3 polynomial fits a cubic curve to the data \n",
    "\n",
    "$$y = ax^3 + bx^2 + cx + d$$\n",
    "\n",
    "In scikit learn, we can implement this using a polynomial preprocessor which translate data into its polynomials.\n",
    "\n",
    "For example, if our x is \n",
    "\n",
    "<code>x = np.array([1, 2, 3, 4, 5])</code>\n",
    "\n",
    "If we perform polynomial transformation like this:\n",
    "\n",
    "<code>poly_X = PolynomialFeatures(degree = 3).fit_transform(X)</code>\n",
    "    \n",
    "X2 will look like this:\n",
    "\n",
    "<code>[[ 1, 1, 1]\n",
    " [ 2, 4, 8]\n",
    " [ 3, 9, 27]\n",
    " [ 4, 16, 64]\n",
    " [ 5, 25, 125]]</code>\n",
    " \n",
    " Now our new feature_engineered X has one column representing x, second column representing $x^2$, and third column representing $x^3$.  Now the y becomes \n",
    " \n",
    " $$ y = ax^3 + bx^2 + cx $$ \n",
    " \n",
    "Now let's look at some example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Performing polynomial regression with deg:  1 ======\n",
      "MSE = 2723.98\n",
      "r^2 = 0.488\n",
      "adjusted $r^2$ = 0.476\n",
      "===Performing polynomial regression with deg:  3 ======\n",
      "MSE = 1447967.71\n",
      "r^2 = -271.051\n",
      "adjusted $r^2$ = -277.363\n",
      "===Performing polynomial regression with deg:  5 ======\n",
      "MSE = 1010991.62\n",
      "r^2 = -188.950\n",
      "adjusted $r^2$ = -193.357\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#let's try to fit the model using 1, 3, 5 degrees....\n",
    "for ix, deg in enumerate([1, 3, 5]):\n",
    "    print(\"===Performing polynomial regression with deg: \", deg, \"======\")\n",
    "    model = make_pipeline(PolynomialFeatures(deg), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    \n",
    "    #print(\"Coeff: \", model.named_steps['linearregression'].coef_)\n",
    "    \n",
    "    #$(1/n)sigma(y - f(x))^2) where SSE = sigma(y-f(x))^2\n",
    "    print(f\"MSE = {mean_squared_error(y_test, y_hat):.2f}\")\n",
    "    \n",
    "    #measures goodness of fit\n",
    "    #1 - SSE/TSS  where TSS = sigma(y-ymean)^2\n",
    "    #r^2 can be negative, when fit without an intercept\n",
    "    #We ALMOST never fit without the intercept unless\n",
    "    #you are sure your data comes through the origin (0, 0), e.g., height, width, but NOT house value!\n",
    "    #r^2 upper bound is 1, lower bound can be anything\n",
    "    print(f\"r^2 = {r2_score(y_test, y_hat):.3f}\")\n",
    "    \n",
    "    #calculate adjusted rsquare\n",
    "    #take IV into consideration, to balance out possible overfitting\n",
    "    #increases only if new predictor (x) enhances the model\n",
    "    n, p = X.shape[0], X.shape[1]\n",
    "    adjusted_rsqrt = 1-(1-r2_score(y_test, y_hat))*(n-1)/(n-p-1)\n",
    "    print(f\"adjusted $r^2$ = {adjusted_rsqrt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression + Grid Search\n",
    "\n",
    "Let's apply grid search with polynomial regression\n",
    "\n",
    "Note: once again, we avoid doing nested cross-validation to save time...but it's recommended.  It helps you check whether the model or the search space really gives stable and generalized results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'linearregression__fit_intercept': False, 'polynomialfeatures__degree': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "              'linearregression__fit_intercept': [True, False]}\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=3)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "\n",
    "y_hat = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 2723.98\n",
      "r^2 = 0.488\n",
      "adjusted $r^2$ = 0.476\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE = {mean_squared_error(y_test, y_hat):.2f}\")\n",
    "print(f\"r^2 = {r2_score(y_test, y_hat):.3f}\")\n",
    "n, p = X.shape[0], X.shape[1]\n",
    "adjusted_rsqrt = 1-(1-r2_score(y_test, y_hat))*(n-1)/(n-p-1)\n",
    "print(f\"adjusted $r^2$ = {adjusted_rsqrt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique to alleviate overfitting problem by imposing some penalty to the loss function.\n",
    "\n",
    "### 1. Ridge regression ($L_2$ regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
    "\n",
    "$$ J(\\theta) =  \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n \\boldsymbol{\\theta}_j^2$$\n",
    "\n",
    "where $\\alpha$ is a free parameter that controls the strength of the penalty.\n",
    "This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "params_Ridge = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'ridge__alpha': np.logspace(-1, -4, 10)}\n",
    "\n",
    "##I put normalize=True to reach convergence faster, since it is giving me warnings...as my x value have wide range\n",
    "model = make_pipeline(PolynomialFeatures(), Ridge(normalize=True))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2+ \\lambda\\sum_{j=1}^n |\\theta_j|$$\n",
    "\n",
    "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor *sparse models* where possible: that is, it preferentially sets model coefficients to exactly zero.\n",
    "\n",
    "We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "params_Lasso = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'lasso__alpha': np.logspace(-1, -4, 10)}\n",
    "\n",
    "#put max_iter since it needs more time to reach convergence\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      Lasso(normalize=True, tol=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elastic net \n",
    "\n",
    "Linear regression with combined L1 and L2 regularizer\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^n |\\theta_k| + (1 - \\alpha) \\sum_{k=1}^n \\theta_j^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "#i set tol to be low since it is eating my pc....\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      ElasticNet(normalize=True))\n",
    "\n",
    "#note that sklearn has two parameters, alpha and l1_ratio, for the complete equation, refer to the doc\n",
    "params_Elasticnet = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'elasticnet__alpha': np.logspace(-1, -4, 10),\n",
    "                \"elasticnet__l1_ratio\": np.linspace(0, 1, 5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge or Lasso or Elastic net??\n",
    "\n",
    "Regularization should be ALMOST ALWAYS used, since these techniques reduces overfitting.\n",
    "\n",
    "How to choose is a little bit difficult. It is easier to understand the assumptions behind.\n",
    "1.  Ridge assumes that coefficients are normally distributed.   **Thus, if you don't want any feature to dominate too much, use Ridge.**\n",
    "2. Lasso assumes that coefficients are Laplace distributed (in layman sense, it mean some predictors are very useful while some are completely irrelevant).   Here, Lasso has the ability to shrink coefficient to zero thus eliminate predictors that are not useful to the output, thus automatic feature selection.  **In simple words, if you have only very few predictors with medium/large effect, use Lasso.**\n",
    "3.  Elastic basically is a compromise between the two, and thus take huge computation time to reach that compromise.  **If you have the resource to spare, you can use Elastic net**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ElasticNet + Stochastic Gradient Descent\n",
    "\n",
    "Sklearn provides ElasticNet along with stochastic gradient descent, and they called <code>SGDRegressor()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      SGDRegressor())\n",
    "\n",
    "params_SGD = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'sgdregressor__alpha': np.logspace(-1, -4, 10),\n",
    "                'sgdregressor__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                 'sgdregressor__l1_ratio': np.linspace(0, 1, 5),\n",
    "              'sgdregressor__learning_rate': ['constant', 'optimal',\n",
    "                                             'invscaling', 'adaptive']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many more....\n",
    "\n",
    "There are just too many to mention.  It may be nice to read here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model.   Sklearn documentation usually writes very good manual when to use which algorithm.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
