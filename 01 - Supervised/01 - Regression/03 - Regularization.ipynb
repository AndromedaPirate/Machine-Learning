{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is a technique to alleviate overfitting problem by imposing some penalty to the loss function.\n",
    "\n",
    "### 1. Ridge regression ($L_2$ regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
    "\n",
    "$$ J(\\theta) =  \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n \\boldsymbol{\\theta}_j^2$$\n",
    "\n",
    "where $\\alpha$ is a free parameter that controls the strength of the penalty.\n",
    "This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "params_Ridge = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'ridge__alpha': np.logspace(-1, -4, 10)}\n",
    "\n",
    "##I put normalize=True to reach convergence faster, since it is giving me warnings...as my x value have wide range\n",
    "model = make_pipeline(PolynomialFeatures(), Ridge(normalize=True))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2+ \\lambda\\sum_{j=1}^n |\\theta_j|$$\n",
    "\n",
    "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor *sparse models* where possible: that is, it preferentially sets model coefficients to exactly zero.\n",
    "\n",
    "We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "params_Lasso = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'lasso__alpha': np.logspace(-1, -4, 10)}\n",
    "\n",
    "#put max_iter since it needs more time to reach convergence\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      Lasso(normalize=True, tol=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Elastic net \n",
    "\n",
    "Linear regression with combined L1 and L2 regularizer\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^n |\\theta_k| + (1 - \\alpha) \\sum_{k=1}^n \\theta_j^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "#i set tol to be low since it is eating my pc....\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      ElasticNet(normalize=True))\n",
    "\n",
    "#note that sklearn has two parameters, alpha and l1_ratio, for the complete equation, refer to the doc\n",
    "params_Elasticnet = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'elasticnet__alpha': np.logspace(-1, -4, 10),\n",
    "                \"elasticnet__l1_ratio\": np.linspace(0, 1, 5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge or Lasso or Elastic net??\n",
    "\n",
    "Regularization should be ALMOST ALWAYS used, since these techniques reduces overfitting.\n",
    "\n",
    "How to choose is a little bit difficult. It is easier to understand the assumptions behind.\n",
    "1.  Ridge assumes that coefficients are normally distributed.   **Thus, if you don't want any feature to dominate too much, use Ridge.**\n",
    "2. Lasso assumes that coefficients are Laplace distributed (in layman sense, it mean some predictors are very useful while some are completely irrelevant).   Here, Lasso has the ability to shrink coefficient to zero thus eliminate predictors that are not useful to the output, thus automatic feature selection.  **In simple words, if you have only very few predictors with medium/large effect, use Lasso.**\n",
    "3.  Elastic basically is a compromise between the two, and thus take huge computation time to reach that compromise.  **If you have the resource to spare, you can use Elastic net**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ElasticNet + Stochastic Gradient Descent\n",
    "\n",
    "Sklearn provides ElasticNet along with stochastic gradient descent, and they called <code>SGDRegressor()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      SGDRegressor())\n",
    "\n",
    "params_SGD = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'sgdregressor__alpha': np.logspace(-1, -4, 10),\n",
    "                'sgdregressor__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                 'sgdregressor__l1_ratio': np.linspace(0, 1, 5),\n",
    "              'sgdregressor__learning_rate': ['constant', 'optimal',\n",
    "                                             'invscaling', 'adaptive']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many more....\n",
    "\n",
    "There are just too many to mention.  It may be nice to read here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model.   Sklearn documentation usually writes very good manual when to use which algorithm.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
